{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkAssignment.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the sales.csv into separate DataFrame.\n",
    "sales_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(\"sales.txt\")\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+---------+\n",
      "|customer_id|customer_name|email                |age|city     |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi    |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai   |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore|\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad|\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad|\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai  |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata  |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the customer.csv into separate DataFrame.\n",
    "customer_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(\"customer.txt\")\n",
    "customer_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the schema of both DataFrames.\n",
    "sales_df.printSchema()\n",
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first 5 rows from the sales DataFrame.\n",
    "sales_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows in customers DataFrame are:  7\n",
      "No. of columns in customers DataFrame are:  5\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows and columns in the customer DataFrame.\n",
    "row_count = customer_df.count()\n",
    "print(\"No. of rows in customers DataFrame are: \", row_count)\n",
    "\n",
    "col_count = len(customer_df.columns)\n",
    "print(\"No. of columns in customers DataFrame are: \", col_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows from the sales DataFrame based on \n",
    "# customer_id,product,amount,sale_date,region columns\n",
    "sales_df = sales_df.dropDuplicates([\"customer_id\", \"product\", \"amount\", \"sale_date\", \"region\"])\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+---------+\n",
      "|customer_id|customer_name|email                |age|city     |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi    |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai   |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore|\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad|\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad|\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai  |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata  |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where any column in the customer DataFrame has null values.\n",
    "customer_df = customer_df.dropna()\n",
    "customer_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace null values in the amount column of the sales DataFrame with 0.\n",
    "sales_df = sales_df.fillna({'amount': 0})\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+---------+\n",
      "|customer_id|customer_name|email                |age|city     |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi    |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai   |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore|\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad|\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad|\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai  |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata  |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace null values in the email column of the customer DataFrame with the value \"unknown\".\n",
    "customer_df = customer_df.fillna({'email': 'unknown'})\n",
    "customer_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Column Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|discounted_amount|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |40500.0          |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |18000.0          |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |54000.0          |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |13500.0          |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |45000.0          |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |63000.0          |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |49500.0          |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |36000.0          |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |18000.0          |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |13500.0          |\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Add a new column discounted_amount to the sales DataFrame that applies a 10% discount on amount.\n",
    "sales_df = sales_df.withColumn('discounted_amount', col('amount') * 0.9)\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the city column in the customer DataFrame to customer_city.\n",
    "customer_df = customer_df.withColumnRenamed('city', 'customer_city')\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+-----------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|discounted_amount|\n",
      "+--------+-----------+-------+------+----------+-----------------+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|          40500.0|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|          18000.0|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|          54000.0|\n",
      "|       6|        101| Mobile| 15000|2023-05-10|          13500.0|\n",
      "|       1|        101| Laptop| 50000|2023-01-15|          45000.0|\n",
      "|      10|        105| Laptop| 70000|2023-09-25|          63000.0|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|          49500.0|\n",
      "|       5|        105|Desktop| 40000|2023-04-20|          36000.0|\n",
      "|       8|        103| Tablet| 20000|2023-07-05|          18000.0|\n",
      "|       2|        102| Mobile| 15000|2023-02-10|          13500.0|\n",
      "+--------+-----------+-------+------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the region column from the sales DataFrame.\n",
    "drop_region_df = sales_df.drop('region')\n",
    "drop_region_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+---------------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|customer_age_category|\n",
      "+-----------+-------------+--------------------+---+-------------+---------------------+\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|                Youth|\n",
      "|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|                Adult|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|                Adult|\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|                Youth|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|                Youth|\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|                Adult|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|                Adult|\n",
      "+-----------+-------------+--------------------+---+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Create a new column customer_age_category in the customer DataFrame based on age:\n",
    "    # a. \"Youth\" for age < 30\n",
    "    # b. \"Adult\" for 30 <= age < 50\n",
    "    # c. \"Senior\" for age >= 50\n",
    "\n",
    "age_category_df = customer_df.withColumn('customer_age_category', when(col('age') < 30, 'Youth').when((col('age') >= 30) & (col('age') < 50), 'Adult').otherwise('Senior'))\n",
    "age_category_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|discounted_amount|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |54000.0          |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |63000.0          |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |49500.0          |\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the sales DataFrame to show only rows where amount is greater than 50,000.\n",
    "filtered_amount_df = sales_df.filter(sales_df['amount'] > 50000)\n",
    "filtered_amount_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+-------------+\n",
      "|customer_id|customer_name|email                |age|customer_city|\n",
      "+-----------+-------------+---------------------+---+-------------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "+-----------+-------------+---------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the customer DataFrame to show customers aged between 25 and 30.\n",
    "filtered_age_df = customer_df.filter(customer_df['age'].between(25, 30))\n",
    "filtered_age_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|region_count|\n",
      "+-----------+------------+\n",
      "|        101|           2|\n",
      "|        103|           2|\n",
      "|        102|           2|\n",
      "|        105|           1|\n",
      "|        104|           2|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Identify all customers who have made purchases in more than one region.\n",
    "customer_region_count = sales_df.groupBy('customer_id').agg(countDistinct('region').alias('region_count'))\n",
    "customer_region_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+----+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|discounted_amount|rank|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|          40500.0|   1|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|          36000.0|   2|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|          63000.0|   1|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|          54000.0|   2|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|          49500.0|   3|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|          13500.0|   1|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|          13500.0|   1|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|          18000.0|   1|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|          18000.0|   1|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the top 3 sales based on amount for each product.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "window = Window.partitionBy('product').orderBy(col('amount').desc())\n",
    "sales_df.withColumn('rank',dense_rank().over(window)).filter(col('rank') <= 3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|discounted_amount|customer_id|customer_name|email                |age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |40500.0          |104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |18000.0          |103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |54000.0          |102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |13500.0          |101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |45000.0          |101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |63000.0          |105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |49500.0          |104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |36000.0          |105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |18000.0          |103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |13500.0          |102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 17. Perform an inner join between sales and customer DataFrames on customer_id.\n",
    "sales_df.join(customer_df, sales_df.customer_id == customer_df.customer_id, \"inner\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|discounted_amount|customer_id|customer_name|email                |age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |40500.0          |104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |18000.0          |103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |54000.0          |102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |13500.0          |101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |45000.0          |101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |63000.0          |105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |49500.0          |104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |36000.0          |105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |18000.0          |103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |13500.0          |102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Perform a left join to include all records from sales and matching records from customer\n",
    "sales_df.join(customer_df, sales_df.customer_id == customer_df.customer_id, \"left\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|discounted_amount|customer_id|customer_name|email                |age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |13500.0          |101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |45000.0          |101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |54000.0          |102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |13500.0          |102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |18000.0          |103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |18000.0          |103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |40500.0          |104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |49500.0          |104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |63000.0          |105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |36000.0          |105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|NULL    |NULL       |NULL   |NULL  |NULL      |NULL  |NULL             |106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai      |\n",
      "|NULL    |NULL       |NULL   |NULL  |NULL      |NULL  |NULL             |107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata      |\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-----------+-------------+---------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a full outer join between sales and customer DataFrames.\n",
    "sales_df.join(customer_df, sales_df.customer_id == customer_df.customer_id, \"full\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|email               |age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|106        |Vikas Jain   |vikas.jain@email.com|31 |Chennai      |\n",
      "|107        |Amit Roy     |amit.roy@email.com  |35 |Kolkata      |\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify customers who have not made any purchases by performing an anti-join.\n",
    "customer_df.join(sales_df, customer_df.customer_id == sales_df.customer_id, \"leftanti\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|product|total_sales_amount|\n",
      "+-------+------------------+\n",
      "| Laptop|            235000|\n",
      "| Mobile|             30000|\n",
      "| Tablet|             40000|\n",
      "|Desktop|             85000|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "#  Calculate the total sales amount for each product.\n",
    "total_sales_df = sales_df.groupBy('product').agg(sum('amount').alias('total_sales_amount'))\n",
    "total_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       average_age|\n",
      "+------------------+\n",
      "|30.571428571428573|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Find the average age of customers in the customer DataFrame.\n",
    "average_age_df = customer_df.agg(avg('age').alias('average_age'))\n",
    "average_age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max_amount|\n",
      "+----------+\n",
      "|     70000|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|min_amount|\n",
      "+----------+\n",
      "|     15000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Calculate the maximum and minimum sales amounts in the sales DataFrame.\n",
    "max_sales_amount = sales_df.agg(max('amount').alias('max_amount'))\n",
    "min_sales_amount = sales_df.agg(min('amount').alias('min_amount'))\n",
    "\n",
    "max_sales_amount.show()\n",
    "min_sales_amount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|customer_city|customer_count|\n",
      "+-------------+--------------+\n",
      "|    Bangalore|             1|\n",
      "|      Chennai|             1|\n",
      "|       Mumbai|             1|\n",
      "|    Ahmedabad|             1|\n",
      "|      Kolkata|             1|\n",
      "|        Delhi|             1|\n",
      "|    Hyderabad|             1|\n",
      "+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "#  Group the customer DataFrame by customer_city and count the number of customers in \n",
    "# each city.\n",
    "\n",
    "city_customer_count_df = customer_df.groupBy('customer_city').agg(count('customer_id').alias('customer_count'))\n",
    "city_customer_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|discounted_amount|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|          63000.0|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|          54000.0|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|          49500.0|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|          45000.0|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|          40500.0|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|          36000.0|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|          18000.0|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|          18000.0|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|          13500.0|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|          13500.0|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the sales DataFrame by amount in descending order.\n",
    "sorted_sales_df = sales_df.orderBy(col('amount').desc())\n",
    "sorted_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+-------------+\n",
      "|customer_id|customer_name|email                |age|customer_city|\n",
      "+-----------+-------------+---------------------+---+-------------+\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai      |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata      |\n",
      "+-----------+-------------+---------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the customer DataFrame by age in ascending order.\n",
    "sorted_customer_df = customer_df.orderBy(col('age').asc())\n",
    "sorted_customer_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Union Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------------------+---+-------------+\n",
      "|customer_id|customer_name |email                   |age|customer_city|\n",
      "+-----------+--------------+------------------------+---+-------------+\n",
      "|108        |Pooja Joshi   |pooja.joshi@email.com   |33 |Chennai      |\n",
      "|109        |Pooja Joshi   |pooja.joshi@email.com   |35 |Kolkata      |\n",
      "|110        |Vikram Chauhan|vikram.chauhan@email.com|34 |Pune         |\n",
      "|111        |Neha Gupta    |neha.gupta@email.com    |35 |Kolkata      |\n",
      "|112        |Sunita Rao    |sunita.rao@email.com    |28 |Lucknow      |\n",
      "|113        |Sunita Rao    |sunita.rao@email.com    |33 |Lucknow      |\n",
      "|114        |Rakesh Bansal |rakesh.bansal@email.com |28 |Lucknow      |\n",
      "|115        |Sunita Rao    |sunita.rao@email.com    |25 |Chennai      |\n",
      "+-----------+--------------+------------------------+---+-------------+\n",
      "\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|11      |114        |Tablet |70000 |2023-09-13|North |\n",
      "|12      |115        |Mobile |50000 |2023-11-16|West  |\n",
      "|13      |112        |Tablet |60000 |2023-01-04|South |\n",
      "|14      |109        |Desktop|90000 |2023-03-08|East  |\n",
      "|15      |108        |Laptop |80000 |2023-06-25|East  |\n",
      "|16      |113        |Tablet |50000 |2023-01-18|North |\n",
      "|17      |115        |Desktop|80000 |2023-02-05|North |\n",
      "|18      |112        |Laptop |60000 |2023-07-24|South |\n",
      "|19      |110        |Mobile |40000 |2023-07-08|West  |\n",
      "|20      |108        |Tablet |70000 |2023-03-18|East  |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_customer_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(\"new_customer_data.csv\")\n",
    "new_customer_df.show(truncate=False)\n",
    "\n",
    "new_sales_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(\"new_sales_data.csv\")\n",
    "new_sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------------------+---+-------------+\n",
      "|customer_id|customer_name |email                   |age|customer_city|\n",
      "+-----------+--------------+------------------------+---+-------------+\n",
      "|101        |Arun Sharma   |arun.sharma@email.com   |28 |Delhi        |\n",
      "|102        |Meena Verma   |meena.verma@email.com   |34 |Mumbai       |\n",
      "|103        |Rahul Yadav   |rahul.yadav@email.com   |30 |Bangalore    |\n",
      "|104        |Priya Patel   |priya.patel@email.com   |27 |Ahmedabad    |\n",
      "|105        |Sneha Reddy   |sneha.reddy@email.com   |29 |Hyderabad    |\n",
      "|106        |Vikas Jain    |vikas.jain@email.com    |31 |Chennai      |\n",
      "|107        |Amit Roy      |amit.roy@email.com      |35 |Kolkata      |\n",
      "|108        |Pooja Joshi   |pooja.joshi@email.com   |33 |Chennai      |\n",
      "|109        |Pooja Joshi   |pooja.joshi@email.com   |35 |Kolkata      |\n",
      "|110        |Vikram Chauhan|vikram.chauhan@email.com|34 |Pune         |\n",
      "|111        |Neha Gupta    |neha.gupta@email.com    |35 |Kolkata      |\n",
      "|112        |Sunita Rao    |sunita.rao@email.com    |28 |Lucknow      |\n",
      "|113        |Sunita Rao    |sunita.rao@email.com    |33 |Lucknow      |\n",
      "|114        |Rakesh Bansal |rakesh.bansal@email.com |28 |Lucknow      |\n",
      "|115        |Sunita Rao    |sunita.rao@email.com    |25 |Chennai      |\n",
      "+-----------+--------------+------------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform a union operation with the customer DataFrame.\n",
    "union_df_customer=customer_df.union(new_customer_df)\n",
    "union_df_customer.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+----------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|discounted_amount|sales_rank|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----------+\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|          36000.0|         1|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|          40500.0|         2|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|          45000.0|         1|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|          49500.0|         2|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|          54000.0|         3|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|          63000.0|         4|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|          13500.0|         1|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|          13500.0|         1|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|          18000.0|         1|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|          18000.0|         1|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rank the sales records based on the amount column.\n",
    "from pyspark.sql.window import Window\n",
    "window=window.orderBy('amount')\n",
    "rank_sales = sales_df.withColumn(\"sales_rank\", dense_rank().over(window))\n",
    "rank_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+-------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|discounted_amount|cum_sum|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-------+\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|          36000.0|  40000|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|          40500.0|  85000|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|          45000.0|  50000|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|          49500.0| 105000|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|          54000.0| 165000|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|          63000.0| 235000|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|          13500.0|  15000|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|          13500.0|  30000|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|          18000.0|  20000|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|          18000.0|  40000|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a cumulative sum of amount for each product in the sales DataFrame.\n",
    "window = Window.partitionBy(\"product\").orderBy(\"amount\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "cum_sales = sales_df.withColumn(\"cum_sum\", sum(\"amount\").over(window))\n",
    "cum_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+----------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|discounted_amount|difference|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----------+\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|          36000.0|   -2500.0|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|          40500.0|    2500.0|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|          45000.0|   -8750.0|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|          49500.0|   -3750.0|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|          54000.0|    1250.0|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|          63000.0|   11250.0|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|          13500.0|       0.0|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|          13500.0|       0.0|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|          18000.0|       0.0|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|          18000.0|       0.0|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Add a column that calculates the difference between each customer's amount and the \n",
    "# average amount within their product group.\n",
    "window = Window.partitionBy(\"product\").orderBy(\"amount\").rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "diff = sales_df.withColumn(\"difference\", col(\"amount\") - avg(\"amount\").over(window))\n",
    "diff.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sales DataFrame to a partitioned Parquet file by region.\n",
    "sales_df.write.partitionBy(\"region\").mode(\"overwrite\").parquet(\"sales.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the customer DataFrame by customer_city and save it as a CSV file\n",
    "customer_df.write.partitionBy(\"customer_city\").mode(\"overwrite\").csv(\"customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Real-World Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|product| contribution_perc|\n",
      "+-------+------------------+\n",
      "| Laptop|60.256410256410255|\n",
      "| Mobile|7.6923076923076925|\n",
      "| Tablet|10.256410256410257|\n",
      "|Desktop|21.794871794871796|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage contribution of each product to the total sales.\n",
    "total_sales=sales_df.groupBy().sum('amount').collect()[0][0]\n",
    "perc_cont = sales_df.groupBy('product').agg((sum('amount')*100/total_sales).alias('contribution_perc'))\n",
    "\n",
    "perc_cont.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Year|total_sales|\n",
      "+----+-----------+\n",
      "|2023|     390000|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, sum\n",
    "\n",
    "# Extract the year from sale_date and group by year to calculate total sales\n",
    "sales_df = sales_df.withColumn(\"Year\", year(\"sale_date\"))\n",
    "df_grouped_by_year = sales_df.groupBy(\"Year\").agg(sum(\"amount\").alias(\"total_sales\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|region|product|\n",
      "+------+-------+\n",
      "|  East| Laptop|\n",
      "| North| Laptop|\n",
      "| South| Mobile|\n",
      "|  West|Desktop|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the most purchased product in each region.\n",
    "\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "df_most_purchased =  spark.sql(\"\"\"\n",
    "    SELECT region, product\n",
    "    FROM (\n",
    "        SELECT region, product, SUM(amount) as TotalAmount,\n",
    "               ROW_NUMBER() OVER (PARTITION BY region ORDER BY SUM(amount) DESC) as Rank\n",
    "        FROM sales\n",
    "        GROUP BY region, product\n",
    "    ) tmp\n",
    "    WHERE Rank = 1\n",
    "\"\"\")\n",
    "\n",
    "df_most_purchased.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---------+\n",
      "|product|MaxAmount|MinAmount|sale_diff|\n",
      "+-------+---------+---------+---------+\n",
      "| Laptop|    70000|    50000|    20000|\n",
      "| Mobile|    15000|    15000|        0|\n",
      "| Tablet|    20000|    20000|        0|\n",
      "|Desktop|    45000|    40000|     5000|\n",
      "+-------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Add a column to show the difference between the highest and lowest sales for each \n",
    "# product.\n",
    "\n",
    "grouped_sales_df = sales_df.groupBy(\"product\").agg(max(\"amount\").alias(\"MaxAmount\"),\n",
    "                                             min(\"amount\").alias(\"MinAmount\"))\n",
    "\n",
    "sales_diff_df = grouped_sales_df.withColumn(\"sale_diff\", col(\"MaxAmount\") - col(\"MinAmount\"))\n",
    "sales_diff_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+----+-------+-------------+---------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|discounted_amount|Year|cust_id|customer_name|email                |age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----+-------+-------------+---------------------+---+-------------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |40500.0          |2023|104    |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |18000.0          |2023|103    |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |54000.0          |2023|102    |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |13500.0          |2023|101    |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |45000.0          |2023|101    |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |63000.0          |2023|105    |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |49500.0          |2023|104    |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |36000.0          |2023|105    |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |18000.0          |2023|103    |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |13500.0          |2023|102    |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "+--------+-----------+-------+------+----------+------+-----------------+----+-------+-------------+---------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the result of the join between sales and customer to parquet file\n",
    "\n",
    "customer_df_renamed = customer_df.withColumnRenamed(\"customer_id\", \"cust_id\")\n",
    "\n",
    "sale_customer_joined = sales_df.join(customer_df_renamed, sales_df['customer_id'] == customer_df_renamed['cust_id'], 'inner')\n",
    "\n",
    "sale_customer_joined.show(truncate=False)\n",
    "\n",
    "sale_customer_joined.write.mode('overwrite').parquet('sales_customer_joined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+---------+------+-----------------+----+-----------+\n",
      "|sales_id|customer_id|product|amount|sale_date|region|discounted_amount|Year|months_diff|\n",
      "+--------+-----------+-------+------+---------+------+-----------------+----+-----------+\n",
      "+--------+-----------+-------+------+---------+------+-----------------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between, current_date\n",
    "\n",
    "filtered_sales = sales_df.withColumn('months_diff', months_between(current_date(), col('sale_date'))) \\\n",
    "    .filter(col('months_diff') <= 6)\n",
    "\n",
    "filtered_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|avg_amount|\n",
      "+-----------+----------+\n",
      "|        101|   32500.0|\n",
      "|        102|   37500.0|\n",
      "|        103|   20000.0|\n",
      "|        104|   50000.0|\n",
      "|        105|   55000.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average sales amount per customer.\n",
    "avg_sales = spark.sql(\"select customer_id, avg(amount) as avg_amount from sales group by customer_id order by customer_id\")\n",
    "avg_sales.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
