{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkAssignment') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales=spark.read.format(\"csv\")\\\n",
    "             .option(\"header\",True)\\\n",
    "             .option(\"inferSchema\",True)\\\n",
    "             .load(\"sales.txt\")\n",
    "df_sales.printSchema()\n",
    "df_sales.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|customer_id|customer_name|email                |age|city     |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi    |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai   |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore|\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad|\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad|\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai  |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata  |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer=spark.read.format(\"csv\")\\\n",
    "             .option(\"header\",True)\\\n",
    "             .option(\"inferSchema\",True)\\\n",
    "             .load(\"customer.txt\")\n",
    "df_customer.printSchema()\n",
    "df_customer.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.createOrReplaceTempView('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns: 5\n",
      "number of rows: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"number of columns:\",len(df_customer.columns))\n",
    "print(\"number of rows:\",df_customer.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.dropDuplicates(['customer_id','amount','product','sale_date','region']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.dropna().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.na.fill(0,'amount').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+---------+\n",
      "|customer_id|customer_name|email                |age|city     |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi    |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai   |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore|\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad|\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad|\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai  |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata  |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer.na.fill(\"unknown\",'email').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Column Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|discounted_amount|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|          45000.0|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|          13500.0|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|          18000.0|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|          49500.0|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|          36000.0|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|          13500.0|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|          54000.0|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|          18000.0|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|          40500.0|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|          63000.0|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Add a new column discounted_amount to the sales DataFrame that applies a 10% discount on amount\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_sales.withColumn(\"discounted_amount\",expr(\"amount*0.9\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10.Rename the city column in the customer DataFrame to customer_city\n",
    "\n",
    "df_customer=df_customer.withColumnRenamed(\"city\",\"customer_city\")\n",
    "df_customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+\n",
      "|sales_id|customer_id|product|amount| sale_date|\n",
      "+--------+-----------+-------+------+----------+\n",
      "|       1|        101| Laptop| 50000|2023-01-15|\n",
      "|       2|        102| Mobile| 15000|2023-02-10|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|\n",
      "|       5|        105|Desktop| 40000|2023-04-20|\n",
      "|       6|        101| Mobile| 15000|2023-05-10|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|\n",
      "|       8|        103| Tablet| 20000|2023-07-05|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|\n",
      "|      10|        105| Laptop| 70000|2023-09-25|\n",
      "+--------+-----------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Drop the region column from the sales DataFrame.\n",
    "df_sales.drop(\"region\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+---------------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|customer_age_category|\n",
      "+-----------+-------------+--------------------+---+-------------+---------------------+\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|                Youth|\n",
      "|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|                Adult|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|                Adult|\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|                Youth|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|                Youth|\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|                Adult|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|                Adult|\n",
      "+-----------+-------------+--------------------+---+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12. Create a new column customer_age_category in the customer DataFrame based on age:\n",
    "# a. \"Youth\" for age < 30\n",
    "# b. \"Adult\" for 30 <= age < 50\n",
    "# c. \"Senior\" for age >= 50\n",
    "from pyspark.sql.functions import col ,when\n",
    "df_customer.withColumn(\"customer_age_category\",when(col(\"age\")<30, \"Youth\")\\\n",
    "                                   .when(col(\"age\")>=50,\"Senior\")\\\n",
    "                                   .when((col(\"age\")>=30) & (col(\"age\")<50),\"Adult\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILTERING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13. Filter the sales DataFrame to show only rows where amount is greater than 50,000.\n",
    "df_sales.filter(expr(\"amount>50000\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14. Filter the customer DataFrame to show customers aged between 25 and 30.\n",
    "df_customer.filter(expr(\"age>=25 and age<=30\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|customer_id|new_count|\n",
      "+-----------+---------+\n",
      "|        101|        2|\n",
      "|        103|        2|\n",
      "|        102|        2|\n",
      "|        104|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. Identify all customers who have made purchases in more than one region.\n",
    "from pyspark.sql.functions import count_distinct\n",
    "df_sales.groupBy(\"customer_id\").agg(count_distinct(col(\"region\")).alias(\"new_count\")).filter(col(\"new_count\")>1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+---------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|top_sales|\n",
      "+--------+-----------+-------+------+----------+------+---------+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        1|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        2|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        1|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        2|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        3|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        1|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        1|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        1|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        1|\n",
      "+--------+-----------+-------+------+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 16. Filter the top 3 sales based on amount for each product\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank\n",
    "window=Window.partitionBy(\"product\").orderBy(col(\"amount\").desc())\n",
    "\n",
    "new_df_topsales=df_sales.withColumn(\"top_sales\",dense_rank().over(window)).filter(col(\"top_sales\")<=3).orderBy(\"product\")\n",
    "new_df_topsales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 17. Perform an inner join between sales and customer DataFrames on customer_id.\n",
    "df_sales.join(df_customer,df_sales.customer_id== df_customer.customer_id,\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 18. Perform a left join to include all records from sales and matching records from customer.\n",
    "df_sales.join(df_customer,df_sales.customer_id== df_customer.customer_id,\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|    NULL|       NULL|   NULL|  NULL|      NULL|  NULL|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|    NULL|       NULL|   NULL|  NULL|      NULL|  NULL|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 19. Perform a full outer join between sales and customer DataFrames.\n",
    "df_sales.join(df_customer,df_sales.customer_id== df_customer.customer_id,\"full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 20. Identify customers who have not made any purchases by performing an anti-join.\n",
    "df_customer.join(df_sales,df_sales.customer_id== df_customer.customer_id,\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGGREGATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|product|total_sales|\n",
      "+-------+-----------+\n",
      "| Laptop|     235000|\n",
      "| Mobile|      30000|\n",
      "| Tablet|      40000|\n",
      "|Desktop|      85000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 21. Calculate the total sales amount for each product.\n",
    "from pyspark.sql.functions import *\n",
    "df_sales.groupBy(\"product\").agg(sum(\"amount\").alias(\"total_sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       average_age|\n",
      "+------------------+\n",
      "|30.571428571428573|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 22. Find the average age of customers in the customer DataFrame\n",
    "df_customer.select(avg(\"age\").alias(\"average_age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|maximum_amt|minimum_amt|\n",
      "+-----------+-----------+\n",
      "|      70000|      15000|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 23. Calculate the maximum and minimum sales amounts in the sales DataFrame.\n",
    "df_sales.select(max(\"amount\").alias(\"maximum_amt\"),min(\"amount\").alias(\"minimum_amt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|customer_city|no. of customers|\n",
      "+-------------+----------------+\n",
      "|    Bangalore|               1|\n",
      "|      Chennai|               1|\n",
      "|       Mumbai|               1|\n",
      "|    Ahmedabad|               1|\n",
      "|      Kolkata|               1|\n",
      "|        Delhi|               1|\n",
      "|    Hyderabad|               1|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 24. Group the customer DataFrame by customer_city and count the number of customers in each city\n",
    "df_customer.groupBy(\"customer_city\").agg(count(\"customer_id\").alias(\"no. of customers\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SORTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 25. Sort the sales DataFrame by amount in descending order\n",
    "df_sales.orderBy(col(\"amount\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 26. Sort the customer DataFrame by age in ascending order.\n",
    "df_customer.orderBy(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to new_customer_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new_customer_data = [\n",
    "    [108, 'Pooja Joshi', 'pooja.joshi@email.com', 33, 'Chennai'],\n",
    "    [109, 'Pooja Joshi', 'pooja.joshi@email.com', 35, 'Kolkata'],\n",
    "    [110, 'Vikram Chauhan', 'vikram.chauhan@email.com', 34, 'Pune'],\n",
    "    [111, 'Neha Gupta', 'neha.gupta@email.com', 35, 'Kolkata'],\n",
    "    [112, 'Sunita Rao', 'sunita.rao@email.com', 28, 'Lucknow'],\n",
    "    [113, 'Sunita Rao', 'sunita.rao@email.com', 33, 'Lucknow'],\n",
    "    [114, 'Rakesh Bansal', 'rakesh.bansal@email.com', 28, 'Lucknow'],\n",
    "    [115, 'Sunita Rao', 'sunita.rao@email.com', 25, 'Chennai']\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(new_customer_data, columns=['customer_id','customer_name','email','age','customer_city'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('new_customer_data.csv', index=False)\n",
    "\n",
    "print(\"Data has been successfully saved to new_customer_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully saved to new_sales_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the new sales data\n",
    "new_sales_data = [\n",
    "    [11, 114, 'Tablet', 70000, '2023-09-13','North'],\n",
    "    [12, 115, 'Mobile', 50000, '2023-11-16','West'],\n",
    "    [13, 112, 'Tablet', 60000, '2023-01-04','South'],\n",
    "    [14, 109, 'Desktop', 90000, '2023-03-08','East'],\n",
    "    [15, 108, 'Laptop', 80000, '2023-06-25','East'],\n",
    "    [16, 113, 'Tablet', 50000, '2023-01-18','North'],\n",
    "    [17, 115, 'Desktop', 80000, '2023-02-05','North'],\n",
    "    [18, 112, 'Laptop', 60000, '2023-07-24','South'],\n",
    "    [19, 110, 'Mobile', 40000, '2023-07-08','West'],\n",
    "    [20, 108, 'Tablet', 70000, '2023-03-18','East']\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(new_sales_data, columns=['sales_id','customer_id','product','amount','sale_date','region'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('new_sales_data.csv', index=False)\n",
    "\n",
    "print(\"Data has been successfully saved to new_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNION OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------------------+---+-------------+\n",
      "|customer_id|customer_name |email                   |age|customer_city|\n",
      "+-----------+--------------+------------------------+---+-------------+\n",
      "|108        |Pooja Joshi   |pooja.joshi@email.com   |33 |Chennai      |\n",
      "|109        |Pooja Joshi   |pooja.joshi@email.com   |35 |Kolkata      |\n",
      "|110        |Vikram Chauhan|vikram.chauhan@email.com|34 |Pune         |\n",
      "|111        |Neha Gupta    |neha.gupta@email.com    |35 |Kolkata      |\n",
      "|112        |Sunita Rao    |sunita.rao@email.com    |28 |Lucknow      |\n",
      "|113        |Sunita Rao    |sunita.rao@email.com    |33 |Lucknow      |\n",
      "|114        |Rakesh Bansal |rakesh.bansal@email.com |28 |Lucknow      |\n",
      "|115        |Sunita Rao    |sunita.rao@email.com    |25 |Chennai      |\n",
      "+-----------+--------------+------------------------+---+-------------+\n",
      "\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|11      |114        |Tablet |70000 |2023-09-13|North |\n",
      "|12      |115        |Mobile |50000 |2023-11-16|West  |\n",
      "|13      |112        |Tablet |60000 |2023-01-04|South |\n",
      "|14      |109        |Desktop|90000 |2023-03-08|East  |\n",
      "|15      |108        |Laptop |80000 |2023-06-25|East  |\n",
      "|16      |113        |Tablet |50000 |2023-01-18|North |\n",
      "|17      |115        |Desktop|80000 |2023-02-05|North |\n",
      "|18      |112        |Laptop |60000 |2023-07-24|South |\n",
      "|19      |110        |Mobile |40000 |2023-07-08|West  |\n",
      "|20      |108        |Tablet |70000 |2023-03-18|East  |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_customer_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(\"new_customer_data.csv\")\n",
    "new_customer_df.show(truncate=False)\n",
    "\n",
    "new_sales_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").csv(\"new_sales_data.csv\")\n",
    "new_sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+---+-------------+\n",
      "|customer_id| customer_name|               email|age|customer_city|\n",
      "+-----------+--------------+--------------------+---+-------------+\n",
      "|        101|   Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        102|   Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|        103|   Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        104|   Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        105|   Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|        106|    Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        107|      Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "|        108|   Pooja Joshi|pooja.joshi@email...| 33|      Chennai|\n",
      "|        109|   Pooja Joshi|pooja.joshi@email...| 35|      Kolkata|\n",
      "|        110|Vikram Chauhan|vikram.chauhan@em...| 34|         Pune|\n",
      "|        111|    Neha Gupta|neha.gupta@email.com| 35|      Kolkata|\n",
      "|        112|    Sunita Rao|sunita.rao@email.com| 28|      Lucknow|\n",
      "|        113|    Sunita Rao|sunita.rao@email.com| 33|      Lucknow|\n",
      "|        114| Rakesh Bansal|rakesh.bansal@ema...| 28|      Lucknow|\n",
      "|        115|    Sunita Rao|sunita.rao@email.com| 25|      Chennai|\n",
      "+-----------+--------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_df_customer=df_customer.union(new_customer_df)\n",
    "union_df_customer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WINDOW FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+----------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|sales_rank|\n",
      "+--------+-----------+-------+------+----------+------+----------+\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|         1|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|         1|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|         2|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|         2|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|         3|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|         4|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|         5|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|         6|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|         7|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|         8|\n",
      "+--------+-----------+-------+------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 29. Rank the sales records based on the amount column.\n",
    "from pyspark.sql.window import Window\n",
    "window=Window.orderBy('amount')\n",
    "df_sales.withColumn(\"sales_rank\",dense_rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|sum_amt|\n",
      "+--------+-----------+-------+------+----------+------+-------+\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|  40000|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|  85000|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|  50000|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East| 105000|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East| 165000|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North| 235000|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|  15000|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|  30000|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|  20000|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|  40000|\n",
      "+--------+-----------+-------+------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 30. Add a cumulative sum of amount for each product in the sales DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window=Window.partitionBy(\"product\").orderBy('amount').rowsBetween(Window.unboundedPreceding, 0)\n",
    "df_sales.withColumn(\"sum_amt\",sum(\"amount\").over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+----------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|difference|\n",
      "+--------+-----------+-------+------+----------+------+----------+\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|   -2500.0|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|    2500.0|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|   -8750.0|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|   -3750.0|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|    1250.0|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|   11250.0|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|       0.0|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|       0.0|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|       0.0|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|       0.0|\n",
      "+--------+-----------+-------+------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 31. Add a column that calculates the difference between each customer's amount and the average amount within their product group.\n",
    "window=Window.partitionBy(\"product\").orderBy('amount').rowsBetween(Window.unboundedPreceding,Window.unboundedFollowing)\n",
    "df_sales.withColumn(\"difference\",col(\"amount\")-avg(\"amount\").over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARTITIONING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32. Write the sales DataFrame to a partitioned Parquet file by region.\n",
    "df_sales.write.partitionBy(\"region\").mode(\"overwrite\").parquet(\"sales.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33. Partition the customer DataFrame by customer_city and save it as a CSV file.\n",
    "df_customer.write.partitionBy(\"customer_city\").mode(\"overwrite\").csv(\"customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL-WORLD SCENARIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|product| contribution_perc|\n",
      "+-------+------------------+\n",
      "| Laptop|60.256410256410255|\n",
      "| Mobile|7.6923076923076925|\n",
      "| Tablet|10.256410256410257|\n",
      "|Desktop|21.794871794871796|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 34. Calculate the percentage contribution of each product to the total sales.\n",
    "total_sales=df_sales.groupBy().sum('amount').collect()[0][0]\n",
    "df_sales.groupBy('product').agg((sum('amount')*100/total_sales).alias('contribution_perc'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|total_sales|\n",
      "+----+-----------+\n",
      "|2023|     390000|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the year from sale_date and group by year to calculate total sales.\n",
    "df_sales.groupBy(year('sale_date').alias('year')).agg(sum('amount').alias('total_sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|region|product|\n",
      "+------+-------+\n",
      "|  East| Laptop|\n",
      "| North| Laptop|\n",
      "| South| Mobile|\n",
      "|  West|Desktop|\n",
      "|  West| Tablet|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify the most purchased product in each region.\n",
    "sales_new=df_sales.groupBy('region','product').agg(count('product').alias('prod_count'))\n",
    "window5=Window.partitionBy('region').orderBy(col('prod_count').desc())\n",
    "sales_new.withColumn('most_purchased',dense_rank().over(window5))\\\n",
    "        .filter(col('most_purchased')==1)\\\n",
    "        .select('region','product')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|product|difference|\n",
      "+-------+----------+\n",
      "| Laptop|     20000|\n",
      "| Mobile|         0|\n",
      "| Tablet|         0|\n",
      "|Desktop|      5000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column to show the difference between the highest and lowest sales for each product\n",
    "spark.sql('''\n",
    "        select product,max(amount)-min(amount) as difference\n",
    "        from sales\n",
    "        group by product\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       1| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       2| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       3| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       4| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       5|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       6| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       7| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       8| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       9|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|      10| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "+--------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write the result of the join between sales and customer to parquet file.\n",
    "sale_customer_joined=df_sales.join(df_customer,df_sales['customer_id']==df_customer['customer_id'],'inner')\\\n",
    "    .select('sales_id','product','amount','sale_date','region',df_sales['customer_id'],'customer_name','email','age','customer_city')\n",
    "sale_customer_joined.show()\n",
    "sale_customer_joined.write.mode('overwrite').parquet('sales_customer_joined.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+---------+------+-----------+\n",
      "|sales_id|customer_id|product|amount|sale_date|region|months_diff|\n",
      "+--------+-----------+-------+------+---------+------+-----------+\n",
      "+--------+-----------+-------+------+---------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify products that were sold in the last 6 months.\n",
    "df_sales.withColumn('months_diff',months_between(current_date(),col('sale_date')))\\\n",
    "    .filter(col('months_diff')<=6)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|customer_id|avg_sale|\n",
      "+-----------+--------+\n",
      "|        101| 32500.0|\n",
      "|        102| 37500.0|\n",
      "|        103| 20000.0|\n",
      "|        104| 50000.0|\n",
      "|        105| 55000.0|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average sales amount per customer.\n",
    "spark.sql('''\n",
    "    select customer_id,avg(amount) as avg_sale\n",
    "    from sales\n",
    "    group by customer_id\n",
    "    order by customer_id\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
