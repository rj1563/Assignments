{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://SF-CPU-0233.simform.dom:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_assignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x28213425a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('pyspark_assignment').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Load the sales.csv and customer.csv files into separate DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sales_id: int, customer_id: int, product: string, amount: int, sale_date: date, region: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df=spark.read.format('csv')\\\n",
    "                .option('header',True)\\\n",
    "                .option('inferSchema',True)\\\n",
    "                .load('sales.txt')\n",
    "sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: int, customer_name: string, email: string, age: int, city: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df=spark.read.format('csv')\\\n",
    "                .option('header',True)\\\n",
    "                .option('inferSchema',True)\\\n",
    "                .load('customer.txt')\n",
    "customer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Display the schema of both DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sales_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- sale_date: date (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Show the first 5 rows from the sales DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sales_id=1, customer_id=101, product='Laptop', amount=50000, sale_date=datetime.date(2023, 1, 15), region='North'),\n",
       " Row(sales_id=2, customer_id=102, product='Mobile', amount=15000, sale_date=datetime.date(2023, 2, 10), region='South'),\n",
       " Row(sales_id=3, customer_id=103, product='Tablet', amount=20000, sale_date=datetime.date(2023, 3, 5), region='West'),\n",
       " Row(sales_id=4, customer_id=104, product='Laptop', amount=55000, sale_date=datetime.date(2023, 3, 15), region='East'),\n",
       " Row(sales_id=5, customer_id=105, product='Desktop', amount=40000, sale_date=datetime.date(2023, 4, 20), region='North')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Count the number of rows and columns in the customer DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of rows = 7\n",
      "no. of columns = 5\n"
     ]
    }
   ],
   "source": [
    "print('no. of rows =',customer_df.count())\n",
    "print('no. of columns =',len(customer_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Remove duplicate rows from the sales DataFrame based on customer_id,product,amount,sale_date,region columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df=sales_df.dropDuplicates(['customer_id','product','amount','sale_date','region'])\n",
    "sales_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Drop rows where any column in the customer DataFrame has null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+---------+\n",
      "|customer_id|customer_name|email                |age|city     |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi    |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai   |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore|\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad|\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad|\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai  |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata  |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.na.drop(how='any').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Replace null values in the amount column of the sales DataFrame with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.na.fill(0,'amount').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Replace null values in the email column of the customer DataFrame with the value \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+---------+\n",
      "|customer_id|customer_name|email                |age|city     |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi    |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai   |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore|\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad|\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad|\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai  |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata  |\n",
      "+-----------+-------------+---------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.na.fill('unknown','email').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Column Manipulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. Add a new column discounted_amount to the sales DataFrame that applies a 10% discount on amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|sales_id|customer_id|product|amount|sale_date |region|discounted_amount|\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "|9       |104        |Desktop|45000 |2023-08-10|West  |40500.0          |\n",
      "|3       |103        |Tablet |20000 |2023-03-05|West  |18000.0          |\n",
      "|7       |102        |Laptop |60000 |2023-06-15|East  |54000.0          |\n",
      "|6       |101        |Mobile |15000 |2023-05-10|South |13500.0          |\n",
      "|1       |101        |Laptop |50000 |2023-01-15|North |45000.0          |\n",
      "|10      |105        |Laptop |70000 |2023-09-25|North |63000.0          |\n",
      "|4       |104        |Laptop |55000 |2023-03-15|East  |49500.0          |\n",
      "|5       |105        |Desktop|40000 |2023-04-20|North |36000.0          |\n",
      "|8       |103        |Tablet |20000 |2023-07-05|North |18000.0          |\n",
      "|2       |102        |Mobile |15000 |2023-02-10|South |13500.0          |\n",
      "+--------+-----------+-------+------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.withColumn('discounted_amount',col('amount')-(col('amount')*0.1))\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Rename the city column in the customer DataFrame to customer_city.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+-------------+\n",
      "|customer_id|customer_name|email                |age|customer_city|\n",
      "+-----------+-------------+---------------------+---+-------------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai      |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata      |\n",
      "+-----------+-------------+---------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df=customer_df.withColumnRenamed('city','customer_city')\n",
    "customer_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. Drop the region column from the sales DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+\n",
      "|sales_id|customer_id|product|amount| sale_date|\n",
      "+--------+-----------+-------+------+----------+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|\n",
      "|       6|        101| Mobile| 15000|2023-05-10|\n",
      "|       1|        101| Laptop| 50000|2023-01-15|\n",
      "|      10|        105| Laptop| 70000|2023-09-25|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|\n",
      "|       5|        105|Desktop| 40000|2023-04-20|\n",
      "|       8|        103| Tablet| 20000|2023-07-05|\n",
      "|       2|        102| Mobile| 15000|2023-02-10|\n",
      "+--------+-----------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.drop('region').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Create a new column customer_age_category in the customer DataFrame based on age:\n",
    "    a. \"Youth\" for age < 30\n",
    "    b. \"Adult\" for 30 <= age < 50\n",
    "    c. \"Senior\" for age >= 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------------+---+-------------+---------------------+\n",
      "|customer_id|customer_name|email                |age|customer_city|customer_age_category|\n",
      "+-----------+-------------+---------------------+---+-------------+---------------------+\n",
      "|101        |Arun Sharma  |arun.sharma@email.com|28 |Delhi        |Youth                |\n",
      "|102        |Meena Verma  |meena.verma@email.com|34 |Mumbai       |Adult                |\n",
      "|103        |Rahul Yadav  |rahul.yadav@email.com|30 |Bangalore    |Adult                |\n",
      "|104        |Priya Patel  |priya.patel@email.com|27 |Ahmedabad    |Youth                |\n",
      "|105        |Sneha Reddy  |sneha.reddy@email.com|29 |Hyderabad    |Youth                |\n",
      "|106        |Vikas Jain   |vikas.jain@email.com |31 |Chennai      |Adult                |\n",
      "|107        |Amit Roy     |amit.roy@email.com   |35 |Kolkata      |Adult                |\n",
      "+-----------+-------------+---------------------+---+-------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.withColumn('customer_age_category',when(col('age')<30,'Youth')\n",
    "                                            .when((col('age')>=30) & (col('age')<50),'Adult')\n",
    "                                            .when(col('age')>=50,'Senior'))\\\n",
    ".show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. Filter the sales DataFrame to show only rows where amount is greater than 50,000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_amount=sales_df.filter(col('amount')>50000)\n",
    "filtered_amount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Filter the customer DataFrame to show customers aged between 25 and 30.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySQL\n",
    "customer_df.createOrReplaceTempView('customers')\n",
    "\n",
    "spark.sql('''\n",
    "        select customer_id,customer_name,email,age,customer_city\n",
    "        from customers\n",
    "        where age between 25 and 30 \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Identify all customers who have made purchases in more than one region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|region_count|\n",
      "+-----------+------------+\n",
      "|        101|           2|\n",
      "|        103|           2|\n",
      "|        102|           2|\n",
      "|        104|           2|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy('customer_id')\\\n",
    "    .agg(count_distinct('region').alias('region_count'))\\\n",
    "    .filter(col('region_count')>1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Filter the top 3 sales based on amount for each product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+----+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|rank|\n",
      "+--------+-----------+-------+------+----------+------+----+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|   1|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|   2|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|   1|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|   2|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|   3|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|   1|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|   1|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|   1|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|   1|\n",
      "+--------+-----------+-------+------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window=Window.partitionBy('product').orderBy(col('amount').desc())\n",
    "\n",
    "sales_df.withColumn('rank',dense_rank().over(window)).filter(col('rank')<=3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. Perform an inner join between sales and customer DataFrames on customer_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(customer_df,sales_df.customer_id==customer_df.customer_id,'inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. Perform a left join to include all records from sales and matching records from customer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(customer_df,sales_df.customer_id==customer_df.customer_id,'left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Perform a full outer join between sales and customer DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|    NULL|       NULL|   NULL|  NULL|      NULL|  NULL|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|    NULL|       NULL|   NULL|  NULL|      NULL|  NULL|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n",
      "using PySQL.....\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|    NULL|       NULL|   NULL|  NULL|      NULL|  NULL|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|    NULL|       NULL|   NULL|  NULL|      NULL|  NULL|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+--------+-----------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.join(customer_df,sales_df.customer_id==customer_df.customer_id,'full_outer').show()\n",
    "\n",
    "# done similar using SQL query\n",
    "print('using PySQL.....')\n",
    "customer_df.createOrReplaceTempView('customers')\n",
    "sales_df.createOrReplaceTempView('sales')\n",
    "spark.sql('''\n",
    "    select *\n",
    "    from sales s\n",
    "    full join customers c on c.customer_id=s.customer_id\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. Identify customers who have not made any purchases by performing an anti-join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.join(sales_df,customer_df.customer_id==sales_df.customer_id,'left_anti').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Aggregations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. Calculate the total sales amount for each product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|product|total_amount|\n",
      "+-------+------------+\n",
      "| Laptop|      235000|\n",
      "| Mobile|       30000|\n",
      "| Tablet|       40000|\n",
      "|Desktop|       85000|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy(col('product')).agg(sum('amount').alias('total_amount')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Find the average age of customers in the customer DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|avg_age_of_customers|\n",
      "+--------------------+\n",
      "|  30.571428571428573|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.agg(avg(col('age')).alias('avg_age_of_customers')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Calculate the maximum and minimum sales amounts in the sales DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|min_sale|max_sale|\n",
      "+--------+--------+\n",
      "|   15000|   70000|\n",
      "+--------+--------+\n",
      "\n",
      "using PySQL...\n",
      "+--------+--------+\n",
      "|min_sale|max_sale|\n",
      "+--------+--------+\n",
      "|   15000|   70000|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.agg(min(col('amount')).alias('min_sale'),\n",
    "             max(col('amount')).alias('max_sale'))\\\n",
    "        .show()\n",
    "\n",
    "# using PySQL\n",
    "print('using PySQL...')\n",
    "sales_df.createOrReplaceTempView('sales')\n",
    "spark.sql('''\n",
    "        select min(amount) as min_sale,\n",
    "                max(amount) as max_sale\n",
    "        from sales\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Group the customer DataFrame by customer_city and count the number of customers in each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|customer_city|customer_per_city|\n",
      "+-------------+-----------------+\n",
      "|    Bangalore|                1|\n",
      "|      Chennai|                1|\n",
      "|       Mumbai|                1|\n",
      "|    Ahmedabad|                1|\n",
      "|      Kolkata|                1|\n",
      "|        Delhi|                1|\n",
      "|    Hyderabad|                1|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.groupBy('customer_city').agg(count_distinct('customer_id')\n",
    "                                    .alias('customer_per_city'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Sort the sales DataFrame by amount in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.sort('amount',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Sort the customer DataFrame by age in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|customer_id|customer_name|               email|age|customer_city|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        106|   Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|        107|     Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.orderBy(col('age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Union Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Add a new dataset for customers and perform a union operation with the customer DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customer_data = [\n",
    "    [108, 'Pooja Joshi', 'pooja.joshi@email.com', 33, 'Chennai'],\n",
    "    [109, 'Pooja Joshi', 'pooja.joshi@email.com', 35, 'Kolkata'],\n",
    "    [110, 'Vikram Chauhan', 'vikram.chauhan@email.com', 34, 'Pune'],\n",
    "    [111, 'Neha Gupta', 'neha.gupta@email.com', 35, 'Kolkata'],\n",
    "    [112, 'Sunita Rao', 'sunita.rao@email.com', 28, 'Lucknow'],\n",
    "    [113, 'Sunita Rao', 'sunita.rao@email.com', 33, 'Lucknow'],\n",
    "    [114, 'Rakesh Bansal', 'rakesh.bansal@email.com', 28, 'Lucknow'],\n",
    "    [115, 'Sunita Rao', 'sunita.rao@email.com', 25, 'Chennai']\n",
    "]\n",
    "customer_schema=['customer_id','customer_name','email','age','customer_city']\n",
    "new_customer_df= spark.createDataFrame(data=new_customer_data,schema=customer_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+---+-------------+\n",
      "|customer_id| customer_name|               email|age|customer_city|\n",
      "+-----------+--------------+--------------------+---+-------------+\n",
      "|        101|   Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|        102|   Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|        103|   Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|        104|   Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|        105|   Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|        106|    Vikas Jain|vikas.jain@email.com| 31|      Chennai|\n",
      "|        107|      Amit Roy|  amit.roy@email.com| 35|      Kolkata|\n",
      "|        108|   Pooja Joshi|pooja.joshi@email...| 33|      Chennai|\n",
      "|        109|   Pooja Joshi|pooja.joshi@email...| 35|      Kolkata|\n",
      "|        110|Vikram Chauhan|vikram.chauhan@em...| 34|         Pune|\n",
      "|        111|    Neha Gupta|neha.gupta@email.com| 35|      Kolkata|\n",
      "|        112|    Sunita Rao|sunita.rao@email.com| 28|      Lucknow|\n",
      "|        113|    Sunita Rao|sunita.rao@email.com| 33|      Lucknow|\n",
      "|        114| Rakesh Bansal|rakesh.bansal@ema...| 28|      Lucknow|\n",
      "|        115|    Sunita Rao|sunita.rao@email.com| 25|      Chennai|\n",
      "+-----------+--------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_df.union(new_customer_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28. Combine the sales DataFrame with another DataFrame containing additional sales records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sales_data = [\n",
    "    [11, 114, 'Tablet', 70000, '2023-09-13','North'],\n",
    "    [12, 115, 'Mobile', 50000, '2023-11-16','West'],\n",
    "    [13, 112, 'Tablet', 60000, '2023-01-04','South'],\n",
    "    [14, 109, 'Desktop', 90000, '2023-03-08','East'],\n",
    "    [15, 108, 'Laptop', 80000, '2023-06-25','East'],\n",
    "    [16, 113, 'Tablet', 50000, '2023-01-18','North'],\n",
    "    [17, 115, 'Desktop', 80000, '2023-02-05','North'],\n",
    "    [18, 112, 'Laptop', 60000, '2023-07-24','South'],\n",
    "    [19, 110, 'Mobile', 40000, '2023-07-08','West'],\n",
    "    [20, 108, 'Tablet', 70000, '2023-03-18','East']\n",
    "]\n",
    "sales_schema=['sales_id','customer_id','product','amount','sale_date','region']\n",
    "new_sales_df= spark.createDataFrame(data=new_sales_data,schema=sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|\n",
      "|      11|        114| Tablet| 70000|2023-09-13| North|\n",
      "|      12|        115| Mobile| 50000|2023-11-16|  West|\n",
      "|      13|        112| Tablet| 60000|2023-01-04| South|\n",
      "|      14|        109|Desktop| 90000|2023-03-08|  East|\n",
      "|      15|        108| Laptop| 80000|2023-06-25|  East|\n",
      "|      16|        113| Tablet| 50000|2023-01-18| North|\n",
      "|      17|        115|Desktop| 80000|2023-02-05| North|\n",
      "|      18|        112| Laptop| 60000|2023-07-24| South|\n",
      "|      19|        110| Mobile| 40000|2023-07-08|  West|\n",
      "|      20|        108| Tablet| 70000|2023-03-18|  East|\n",
      "+--------+-----------+-------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df=sales_df.union(new_sales_df)\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Window Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 29. Rank the sales records based on the amount column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+--------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|rank_on_amount|\n",
      "+--------+-----------+-------+------+----------+------+--------------+\n",
      "|      14|        109|Desktop| 90000|2023-03-08|  East|             1|\n",
      "|      15|        108| Laptop| 80000|2023-06-25|  East|             2|\n",
      "|      17|        115|Desktop| 80000|2023-02-05| North|             2|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|             3|\n",
      "|      11|        114| Tablet| 70000|2023-09-13| North|             3|\n",
      "|      20|        108| Tablet| 70000|2023-03-18|  East|             3|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|             4|\n",
      "|      13|        112| Tablet| 60000|2023-01-04| South|             4|\n",
      "|      18|        112| Laptop| 60000|2023-07-24| South|             4|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|             5|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|             6|\n",
      "|      12|        115| Mobile| 50000|2023-11-16|  West|             6|\n",
      "|      16|        113| Tablet| 50000|2023-01-18| North|             6|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|             7|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|             8|\n",
      "|      19|        110| Mobile| 40000|2023-07-08|  West|             8|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|             9|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|             9|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|            10|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|            10|\n",
      "+--------+-----------+-------+------+----------+------+--------------+\n",
      "\n",
      "using PySQL.....\n",
      "+--------+-----------+-------+------+----------+------+--------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|rank_of_amount|\n",
      "+--------+-----------+-------+------+----------+------+--------------+\n",
      "|      14|        109|Desktop| 90000|2023-03-08|  East|             1|\n",
      "|      15|        108| Laptop| 80000|2023-06-25|  East|             2|\n",
      "|      17|        115|Desktop| 80000|2023-02-05| North|             2|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|             3|\n",
      "|      11|        114| Tablet| 70000|2023-09-13| North|             3|\n",
      "|      20|        108| Tablet| 70000|2023-03-18|  East|             3|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|             4|\n",
      "|      13|        112| Tablet| 60000|2023-01-04| South|             4|\n",
      "|      18|        112| Laptop| 60000|2023-07-24| South|             4|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|             5|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|             6|\n",
      "|      12|        115| Mobile| 50000|2023-11-16|  West|             6|\n",
      "|      16|        113| Tablet| 50000|2023-01-18| North|             6|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|             7|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|             8|\n",
      "|      19|        110| Mobile| 40000|2023-07-08|  West|             8|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|             9|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|             9|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|            10|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|            10|\n",
      "+--------+-----------+-------+------+----------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window2=Window.orderBy(col('amount').desc())\n",
    "sales_df.withColumn('rank_on_amount',dense_rank().over(window2)).show()\n",
    "\n",
    "# using Pyspark SQL queryy\n",
    "print('using PySQL.....')\n",
    "sales_df.createOrReplaceTempView('sales')\n",
    "spark.sql('''\n",
    "    select *,dense_rank() over(order by amount desc) as rank_of_amount\n",
    "    from sales\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30. Add a cumulative sum of amount for each product in the sales DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+---------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|cummulative_sum|\n",
      "+--------+-----------+-------+------+----------+------+---------------+\n",
      "|      17|        115|Desktop| 80000|2023-02-05| North|          80000|\n",
      "|      14|        109|Desktop| 90000|2023-03-08|  East|         170000|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|         210000|\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|         255000|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|          50000|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|         105000|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|         165000|\n",
      "|      15|        108| Laptop| 80000|2023-06-25|  East|         245000|\n",
      "|      18|        112| Laptop| 60000|2023-07-24| South|         305000|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|         375000|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|          15000|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|          30000|\n",
      "|      19|        110| Mobile| 40000|2023-07-08|  West|          70000|\n",
      "|      12|        115| Mobile| 50000|2023-11-16|  West|         120000|\n",
      "|      13|        112| Tablet| 60000|2023-01-04| South|          60000|\n",
      "|      16|        113| Tablet| 50000|2023-01-18| North|         110000|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|         130000|\n",
      "|      20|        108| Tablet| 70000|2023-03-18|  East|         200000|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|         220000|\n",
      "|      11|        114| Tablet| 70000|2023-09-13| North|         290000|\n",
      "+--------+-----------+-------+------+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window3=Window.partitionBy('product').orderBy('sale_date')\\\n",
    "            .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "sales_df.withColumn('cummulative_sum',sum('amount').over(window3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31. Add a column that calculates the difference between each customer's amount and the average amount within their product group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+----------+------+------------------+-------------------------+\n",
      "|sales_id|customer_id|product|amount| sale_date|region|        avg_amount|difference_amount_and_avg|\n",
      "+--------+-----------+-------+------+----------+------+------------------+-------------------------+\n",
      "|       9|        104|Desktop| 45000|2023-08-10|  West|           63750.0|                 -18750.0|\n",
      "|       5|        105|Desktop| 40000|2023-04-20| North|           63750.0|                 -23750.0|\n",
      "|      14|        109|Desktop| 90000|2023-03-08|  East|           63750.0|                  26250.0|\n",
      "|      17|        115|Desktop| 80000|2023-02-05| North|           63750.0|                  16250.0|\n",
      "|       7|        102| Laptop| 60000|2023-06-15|  East|           62500.0|                  -2500.0|\n",
      "|       1|        101| Laptop| 50000|2023-01-15| North|           62500.0|                 -12500.0|\n",
      "|      10|        105| Laptop| 70000|2023-09-25| North|           62500.0|                   7500.0|\n",
      "|       4|        104| Laptop| 55000|2023-03-15|  East|           62500.0|                  -7500.0|\n",
      "|      15|        108| Laptop| 80000|2023-06-25|  East|           62500.0|                  17500.0|\n",
      "|      18|        112| Laptop| 60000|2023-07-24| South|           62500.0|                  -2500.0|\n",
      "|       6|        101| Mobile| 15000|2023-05-10| South|           30000.0|                 -15000.0|\n",
      "|       2|        102| Mobile| 15000|2023-02-10| South|           30000.0|                 -15000.0|\n",
      "|      12|        115| Mobile| 50000|2023-11-16|  West|           30000.0|                  20000.0|\n",
      "|      19|        110| Mobile| 40000|2023-07-08|  West|           30000.0|                  10000.0|\n",
      "|       3|        103| Tablet| 20000|2023-03-05|  West|48333.333333333336|      -28333.333333333336|\n",
      "|       8|        103| Tablet| 20000|2023-07-05| North|48333.333333333336|      -28333.333333333336|\n",
      "|      11|        114| Tablet| 70000|2023-09-13| North|48333.333333333336|       21666.666666666664|\n",
      "|      13|        112| Tablet| 60000|2023-01-04| South|48333.333333333336|       11666.666666666664|\n",
      "|      16|        113| Tablet| 50000|2023-01-18| North|48333.333333333336|       1666.6666666666642|\n",
      "|      20|        108| Tablet| 70000|2023-03-18|  East|48333.333333333336|       21666.666666666664|\n",
      "+--------+-----------+-------+------+----------+------+------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window4=Window.partitionBy('product')\n",
    "sales_df.withColumn('avg_amount',avg('amount').over(window4))\\\n",
    "        .withColumn('difference_amount_and_avg',col('amount')-col('avg_amount'))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Partitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 32. Write the sales DataFrame to a partitioned Parquet file by region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.write.partitionBy('region')\\\n",
    "    .mode('overwrite')\\\n",
    "    .parquet('sales.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 33. Partition the customer DataFrame by customer_city and save it as a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.write.partitionBy('customer_city')\\\n",
    "    .mode('append')\\\n",
    "    .csv('customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Real-World Scenarios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 34. Calculate the percentage contribution of each product to the total sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|product| contribution_perc|\n",
      "+-------+------------------+\n",
      "| Laptop| 36.05769230769231|\n",
      "| Mobile|11.538461538461538|\n",
      "| Tablet|27.884615384615383|\n",
      "|Desktop| 24.51923076923077|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_sales=sales_df.groupBy().sum('amount').collect()[0][0]\n",
    "sales_df.groupBy('product').agg((sum('amount')*100/total_sales).alias('contribution_perc'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 35. Extract the year from sale_date and group by year to calculate total sales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|total_sales|\n",
      "+----+-----------+\n",
      "|2023|    1040000|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy(year('sale_date').alias('year')).agg(sum('amount').alias('total_sales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 36. Identify the most purchased product in each region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|region|product|\n",
      "+------+-------+\n",
      "|  East| Laptop|\n",
      "| North| Tablet|\n",
      "| South| Mobile|\n",
      "|  West| Mobile|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_new=sales_df.groupBy('region','product').agg(count('product').alias('prod_count'))\n",
    "window5=Window.partitionBy('region').orderBy(col('prod_count').desc())\n",
    "sales_new.withColumn('most_purchased',dense_rank().over(window5))\\\n",
    "        .filter(col('most_purchased')==1)\\\n",
    "        .select('region','product')\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 37. Add a column to show the difference between the highest and lowest sales for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+----------+\n",
      "|product|lowest_sale|highest_sale|difference|\n",
      "+-------+-----------+------------+----------+\n",
      "| Laptop|      50000|       80000|     30000|\n",
      "| Mobile|      15000|       50000|     35000|\n",
      "| Tablet|      20000|       70000|     50000|\n",
      "|Desktop|      40000|       90000|     50000|\n",
      "+-------+-----------+------------+----------+\n",
      "\n",
      "using PySQL....\n",
      "+-------+----------+\n",
      "|product|difference|\n",
      "+-------+----------+\n",
      "| Laptop|     30000|\n",
      "| Mobile|     35000|\n",
      "| Tablet|     50000|\n",
      "|Desktop|     50000|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy('product').agg(min('amount').alias('lowest_sale'),max('amount').alias('highest_sale'))\\\n",
    "    .withColumn('difference',col('highest_sale')-col('lowest_sale'))\\\n",
    "    .show()\n",
    "\n",
    "# using PySQL\n",
    "print('using PySQL....')\n",
    "sales_df.createOrReplaceTempView('sales')\n",
    "spark.sql('''\n",
    "        select product,max(amount)-min(amount) as difference\n",
    "        from sales\n",
    "        group by product\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 38. Write the result of the join between sales and customer to parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|sales_id|product|amount| sale_date|region|customer_id|customer_name|               email|age|customer_city|\n",
      "+--------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "|       9|Desktop| 45000|2023-08-10|  West|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       3| Tablet| 20000|2023-03-05|  West|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       7| Laptop| 60000|2023-06-15|  East|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "|       6| Mobile| 15000|2023-05-10| South|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|       1| Laptop| 50000|2023-01-15| North|        101|  Arun Sharma|arun.sharma@email...| 28|        Delhi|\n",
      "|      10| Laptop| 70000|2023-09-25| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       4| Laptop| 55000|2023-03-15|  East|        104|  Priya Patel|priya.patel@email...| 27|    Ahmedabad|\n",
      "|       5|Desktop| 40000|2023-04-20| North|        105|  Sneha Reddy|sneha.reddy@email...| 29|    Hyderabad|\n",
      "|       8| Tablet| 20000|2023-07-05| North|        103|  Rahul Yadav|rahul.yadav@email...| 30|    Bangalore|\n",
      "|       2| Mobile| 15000|2023-02-10| South|        102|  Meena Verma|meena.verma@email...| 34|       Mumbai|\n",
      "+--------+-------+------+----------+------+-----------+-------------+--------------------+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_customer_joined=sales_df.join(customer_df,sales_df['customer_id']==customer_df['customer_id'],'inner')\\\n",
    "    .select('sales_id','product','amount','sale_date','region',sales_df['customer_id'],'customer_name','email','age','customer_city')\n",
    "sale_customer_joined.show()\n",
    "sale_customer_joined.write.mode('overwrite').parquet('sales_customer_joined.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 39. Identify products that were sold in the last 6 months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------+------+---------+------+-----------+\n",
      "|sales_id|customer_id|product|amount|sale_date|region|months_diff|\n",
      "+--------+-----------+-------+------+---------+------+-----------+\n",
      "+--------+-----------+-------+------+---------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.withColumn('months_diff',months_between(current_date(),col('sale_date')))\\\n",
    "    .filter(col('months_diff')<=6)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 40. Calculate the average sales amount per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|customer_id|avg_amount_per_customer|\n",
      "+-----------+-----------------------+\n",
      "|        101|                32500.0|\n",
      "|        102|                37500.0|\n",
      "|        103|                20000.0|\n",
      "|        104|                50000.0|\n",
      "|        105|                55000.0|\n",
      "|        108|                75000.0|\n",
      "|        109|                90000.0|\n",
      "|        110|                40000.0|\n",
      "|        112|                60000.0|\n",
      "|        113|                50000.0|\n",
      "|        114|                70000.0|\n",
      "|        115|                65000.0|\n",
      "+-----------+-----------------------+\n",
      "\n",
      "using PySQL....\n",
      "+-----------+--------+\n",
      "|customer_id|avg_sale|\n",
      "+-----------+--------+\n",
      "|        101| 32500.0|\n",
      "|        102| 37500.0|\n",
      "|        103| 20000.0|\n",
      "|        104| 50000.0|\n",
      "|        105| 55000.0|\n",
      "|        108| 75000.0|\n",
      "|        109| 90000.0|\n",
      "|        110| 40000.0|\n",
      "|        112| 60000.0|\n",
      "|        113| 50000.0|\n",
      "|        114| 70000.0|\n",
      "|        115| 65000.0|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.groupBy('customer_id').agg(avg('amount').alias('avg_amount_per_customer'))\\\n",
    "    .orderBy('customer_id')\\\n",
    "    .show()\n",
    "\n",
    "# \n",
    "print('using PySQL....')\n",
    "spark.sql('''\n",
    "    select customer_id,avg(amount) as avg_sale\n",
    "    from sales\n",
    "    group by customer_id\n",
    "    order by customer_id\n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
